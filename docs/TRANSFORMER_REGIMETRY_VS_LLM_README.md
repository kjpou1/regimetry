
# üìä Transformer Encoder in Regimetry vs. LLM Architectures

This document compares the lightweight Transformer encoder used in the `regimetry` framework with traditional large language models (LLMs) such as **BERT** and **GPT**.

---

## üß† Purpose & Use Case

| Feature       | `regimetry` Transformer Encoder             | LLMs (e.g., BERT, GPT)                               |
| ------------- | ------------------------------------------- | ---------------------------------------------------- |
| **Goal**      | Extract latent *market regime embeddings*   | Generate or classify *natural language*              |
| **Usage**     | Downstream clustering & regime segmentation | Text generation, classification, question answering  |
| **Task Type** | Unsupervised time-series representation     | Supervised pretraining (BERT) / autoregressive (GPT) |

---

## üßÆ Input Format

| Feature         | `regimetry`                               | LLMs                                 |
| --------------- | ----------------------------------------- | ------------------------------------ |
| Input           | `(batch_size, window_size, num_features)` | `(batch_size, seq_len)` (token IDs)  |
| Token Type      | Continuous multivariate features          | Discrete tokens (integers)           |
| Sequence Length | Fixed window size (e.g., 30)              | Typically fixed (e.g., 512 for BERT) |

---

## üîÅ Positional Encoding

| Type       | `regimetry`            | LLMs                             |
| ---------- | ---------------------- | -------------------------------- |
| Sinusoidal | ‚úÖ Supported (default)  | ‚úÖ Common in original Transformer |
| Learnable  | ‚úÖ Supported (optional) | ‚úÖ Standard in BERT/GPT variants  |

---

## üß© Encoder Architecture

| Layer         | `regimetry` Transformer       | LLMs                                    |
| ------------- | ----------------------------- | --------------------------------------- |
| Attention     | MultiHead Self-Attention      | MultiHead Self-Attention                |
| Normalization | Post-attention & post-FFN     | Same                                    |
| Feedforward   | Conv1D (1√ó1) FFN blocks       | Dense FFN (linear layers)               |
| Pooling       | ‚úÖ `GlobalAveragePooling1D()`  | ‚ùå BERT uses \[CLS]; GPT uses last token |
| Output        | Fixed-length embedding vector | Token-level or sequence output          |

---

## üõ°Ô∏è Masking Strategy

| Mask Type       | `regimetry`                 | LLMs                           |
| --------------- | --------------------------- | ------------------------------ |
| Padding Mask    | ‚ùå Not needed (no padding)   | ‚úÖ BERT                         |
| Look-Ahead Mask | ‚ùå Not used (not generative) | ‚úÖ GPT                          |
| Rationale       | Fixed-length time windows   | Variable-length, causal models |

---

## üéØ Output Head

| Output Head | `regimetry`                | LLMs                                 |
| ----------- | -------------------------- | ------------------------------------ |
| Purpose     | Embedding for clustering   | Classification (BERT) / logits (GPT) |
| Example     | 256-dim vector via pooling | Vocabulary logits or class labels    |

> üß† *`regimetry` intentionally excludes a supervised output head to focus on **representation learning**, not prediction.*

---

## üìè Model Size & Complexity

| Dimension            | `regimetry` Encoder          | LLMs (e.g., BERT-base, GPT-2) |
| -------------------- | ---------------------------- | ----------------------------- |
| # Transformer Blocks | 2‚Äì4                          | 12‚Äì96                         |
| Parameters           | \~0.5M‚Äì2M                    | 110M (BERT) ‚Üí 175B+ (GPT-3)   |
| Training Data        | None (unsupervised use only) | Massive text corpora          |
| Fine-tuning          | ‚ùå Not required               | ‚úÖ Often required per task     |

---

## ‚úÖ Summary Table

| Feature             | `regimetry` Transformer         | BERT / GPT (LLMs)                |
| ------------------- | ------------------------------- | -------------------------------- |
| Input type          | Continuous time-series          | Discrete text tokens             |
| Attention type      | Self-attention (bidirectional)  | BERT: bidirectional, GPT: causal |
| Output              | Vector embedding (unsupervised) | Class logits / tokens            |
| Masking             | None                            | Padding / causal masks           |
| Positional encoding | Sinusoidal or learnable         | Usually learnable                |
| Pooling             | Global average                  | \[CLS] or final token            |
| Output head         | ‚ùå None                          | ‚úÖ Present                        |
| Size & depth        | Shallow (2‚Äì4 layers)            | Deep (12‚Äì96+ layers)             |

---

## üß≠ Design Philosophy

**`regimetry`** uses Transformer blocks not for language modeling but for **stateful regime representation**.

* Lightweight and interpretable
* Optimized for unsupervised embedding quality
* Avoids complexity of generative decoding or task-specific heads
* Outputs are clustered post hoc (e.g., Spectral Clustering)
