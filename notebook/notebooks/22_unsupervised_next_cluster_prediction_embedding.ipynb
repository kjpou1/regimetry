{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841a8fa1",
   "metadata": {},
   "source": [
    "## ðŸ§­ GOAL:\n",
    "Train a self-supervised model to forecast E[t+1] from [E[t-n], ..., E[t]], then map the forecasted embedding to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6678c38",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## âœ… STEP-BY-STEP OUTLINE\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Load Embeddings**\n",
    "\n",
    "* Load `embeddings.csv` into a NumPy array or pandas DataFrame.\n",
    "* Confirm shape: `(num_timesteps, embedding_dim)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Build Sliding Window Dataset**\n",
    "\n",
    "Create training samples:\n",
    "\n",
    "* Input: `X[i] = [E[i], E[i+1], ..., E[i+window_size-1]]`\n",
    "* Target: `y[i] = E[i+window_size]`\n",
    "\n",
    "This results in:\n",
    "\n",
    "* `X`: shape `(num_samples, window_size, embedding_dim)`\n",
    "* `y`: shape `(num_samples, embedding_dim)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Train/Test Split (Optional)**\n",
    "\n",
    "* Use 80/20 split (or time-ordered split) for validation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Define LSTM Model in Keras**\n",
    "\n",
    "* Use `Sequential` API:\n",
    "\n",
    "  * LSTM layer â†’ Dense output\n",
    "* Compile with:\n",
    "\n",
    "  * Loss: `'mse'`\n",
    "  * Optimizer: `'adam'`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Train the Model**\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=..., batch_size=...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Predict Next Embedding**\n",
    "\n",
    "* Pick a time slice (e.g. last 5 embeddings)\n",
    "* Run `model.predict()` to get `EÌ‚[t+1]`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Assign Cluster to Predicted Embedding**\n",
    "\n",
    "* Scale `EÌ‚[t+1]` using `StandardScaler` (same one from original clustering)\n",
    "* Pass into `KNN.predict([EÌ‚_scaled])` to get `Cluster_ID[t+1]`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Evaluate (Optional)**\n",
    "\n",
    "* Compare predicted cluster with actual `Cluster_ID[t+1]`\n",
    "* Compute:\n",
    "\n",
    "  * Accuracy\n",
    "  * Confusion matrix\n",
    "  * Embedding prediction error (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    "    Lambda,\n",
    "    Bidirectional,\n",
    "    Input,\n",
    "    MultiHeadAttention,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from regimetry.config import Config\n",
    "from regimetry.logger_manager import LoggerManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ac07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = LoggerManager.get_logger(\"unsupervised_next_cluster_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cf7d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT ROOT: /Users/kenneth/Public/projects/python/ai/regimetry\n",
      "BASE DIR: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts\n",
      "RAW DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/data/raw\n",
      "PROCESSED DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/data/processed\n",
      "EMBEDDINGS DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/embeddings\n",
      "REPORTS DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/reports\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "print(\"PROJECT ROOT:\", cfg.PROJECT_ROOT)\n",
    "print(\"BASE DIR:\", cfg.BASE_DIR)\n",
    "print(\"RAW DATA:\", cfg.RAW_DATA_DIR)\n",
    "print(\"PROCESSED DATA:\", cfg.PROCESSED_DATA_DIR)\n",
    "print(\"EMBEDDINGS DATA:\", cfg.EMBEDDINGS_DIR) \n",
    "print(\"REPORTS DATA:\", cfg.REPORTS_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9fc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = path.join(cfg.EMBEDDINGS_DIR, \"GBP_USD/ws5/learnable64_default\")\n",
    "cluster_path = path.join(\n",
    "    cfg.REPORTS_DIR, \"GBP_USD/ws5/learnable64/default/nc8\", \"cluster_assignments.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffd90c",
   "metadata": {},
   "source": [
    "### **Step 1: Load Embeddings**\n",
    "\n",
    "* Load `embeddings.csv` into a NumPy array or pandas DataFrame.\n",
    "* Confirm shape: `(num_timesteps, embedding_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43fc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded embeddings with shape: (1879, 64)\n"
     ]
    }
   ],
   "source": [
    "embedding_file = path.join(\n",
    "    embedding_path,\n",
    "    \"embedding.npy\",\n",
    ")\n",
    "# ðŸ§¾ Load as NumPy array\n",
    "embeddings = np.load(embedding_file)\n",
    "\n",
    "# âœ… Confirm shape\n",
    "num_timesteps, embedding_dim = embeddings.shape\n",
    "print(f\"âœ… Loaded embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a001b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = path.join(\n",
    "    embedding_path,\n",
    "    \"embedding_metadata.json\",\n",
    ")\n",
    "\n",
    "with open(metadata_file) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# print(json.dumps(metadata, indent=2))\n",
    "assert embeddings.shape == (metadata[\"n_samples\"], metadata[\"embedding_dim\"])\n",
    "# ðŸ”§ Set window size\n",
    "window_size = metadata[\"window_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ebb7b",
   "metadata": {},
   "source": [
    "### **Step 2: Build Sliding Window Dataset**\n",
    "\n",
    "Create training samples:\n",
    "\n",
    "* Input: `X[i] = [E[i], E[i+1], ..., E[i+window_size-1]]`\n",
    "* Target: `y[i] = E[i+window_size]`\n",
    "\n",
    "This results in:\n",
    "\n",
    "* `X`: shape `(num_samples, window_size, embedding_dim)`\n",
    "* `y`: shape `(num_samples, embedding_dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5ac7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1874, 5, 64)  (samples, window_size, embedding_dim)\n",
      "y shape: (1874, 64)  (samples, embedding_dim)\n"
     ]
    }
   ],
   "source": [
    "embeddings = normalize(embeddings, norm=\"l2\", axis=1)\n",
    "def build_embedding_forecast_dataset(embeddings, window_size=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(embeddings) - window_size):\n",
    "        X.append(embeddings[i : i + window_size])\n",
    "        y.append(embeddings[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# âš™ï¸ Build dataset\n",
    "X, y = build_embedding_forecast_dataset(embeddings, window_size)\n",
    "\n",
    "# âœ… Confirm shape\n",
    "print(f\"X shape: {X.shape}  (samples, window_size, embedding_dim)\")\n",
    "print(f\"y shape: {y.shape}  (samples, embedding_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbe608",
   "metadata": {},
   "source": [
    "#### Integrity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8f8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All dataset integrity checks passed.\n"
     ]
    }
   ],
   "source": [
    "# From metadata\n",
    "expected_window_size = metadata[\"window_size\"]\n",
    "expected_embedding_dim = metadata[\"embedding_dim\"]\n",
    "expected_num_samples = metadata[\"n_samples\"]\n",
    "\n",
    "# 1. Validate embedding dimensions\n",
    "assert embeddings.shape == (\n",
    "    expected_num_samples,\n",
    "    expected_embedding_dim,\n",
    "), f\"âŒ Embeddings shape mismatch: expected ({expected_num_samples}, {expected_embedding_dim}), got {embeddings.shape}\"\n",
    "\n",
    "# 2. Validate window size\n",
    "assert (\n",
    "    X.shape[1] == expected_window_size\n",
    "), f\"âŒ Window size mismatch: expected {expected_window_size}, got {X.shape[1]}\"\n",
    "\n",
    "# 3. Validate embedding dimension in X and y\n",
    "assert (\n",
    "    X.shape[2] == expected_embedding_dim\n",
    "), f\"âŒ Embedding dim in X mismatch: expected {expected_embedding_dim}, got {X.shape[2]}\"\n",
    "assert (\n",
    "    y.shape[1] == expected_embedding_dim\n",
    "), f\"âŒ Embedding dim in y mismatch: expected {expected_embedding_dim}, got {y.shape[1]}\"\n",
    "\n",
    "# 4. Sanity check on total samples\n",
    "expected_num_sequences = expected_num_samples - expected_window_size\n",
    "assert (\n",
    "    X.shape[0] == expected_num_sequences == y.shape[0]\n",
    "), f\"âŒ Sequence count mismatch: expected {expected_num_sequences}, got X: {X.shape[0]}, y: {y.shape[0]}\"\n",
    "\n",
    "print(\"âœ… All dataset integrity checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306720",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 3 (Optional): Chronological Train/Validation Split\n",
    "\n",
    "### ðŸŽ¯ Purpose\n",
    "\n",
    "Split your time-series embedding dataset into:\n",
    "\n",
    "* `train` set: to learn embedding dynamics\n",
    "* `validation` set: to **monitor overfitting**, **tune architecture**, and **evaluate generalization**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Do It\n",
    "\n",
    "* You want to track model performance realistically\n",
    "* You plan to compare models or tune hyperparameters\n",
    "* You're preparing the model for production use or scientific evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš« When It's Safe to Skip (Temporarily)\n",
    "\n",
    "* You just want to test if the LSTM can **produce a meaningful `EÌ‚[t+1]`**\n",
    "* Youâ€™re validating the **cluster assignment logic**, not the embedding quality\n",
    "* Youâ€™ll circle back later to do formal evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617146ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a005a",
   "metadata": {},
   "source": [
    "### **Step 4: Define LSTM Model in Keras**\n",
    "to predict `E[t+1]` from the previous `window_size` embeddings.\n",
    "\n",
    "* Use `Sequential` API:\n",
    "\n",
    "  * LSTM layer â†’ Dense output\n",
    "* Compile with:\n",
    "\n",
    "  * Loss: `'mse'`\n",
    "  * Optimizer: `'adam'`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482229a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ” What This Does\n",
    "\n",
    "| Layer              | Purpose                                     |\n",
    "| ------------------ | ------------------------------------------- |\n",
    "| `LSTM(64)`         | Learns sequence pattern in embedding flow   |\n",
    "| `Dense(64)`        | Outputs the next predicted embedding vector |\n",
    "| Loss = `MSE`       | Because embeddings are continuous vectors   |\n",
    "| Optimizer = `Adam` | Well-suited for smooth loss landscapes      |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Result\n",
    "\n",
    "You now have a trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31192889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Choose loss function\n",
    "USE_COSINE_LOSS = True\n",
    "USE_HYBRID_LOSS = False  # Toggle this\n",
    "USE_VALIDATION = False  # Toggle this\n",
    "USE_BIG_MAMA_MODEL = True  # ðŸ” Toggle this to switch architectures\n",
    "USE_OUTPUT_NORMALIZATION = True  # Controls whether final output is L2-normalized\n",
    "EPOCHS = 300  # bump it up\n",
    "PATIENCE = 20  # give EarlyStopping some breathing room\n",
    "\n",
    "# Supported model types:\n",
    "#     - 'simple': Single-layer LSTM\n",
    "#     - 'stratum': Stacked LSTM + dropout\n",
    "#     - 'stratum_bi': BiLSTM + LSTM + dropout\n",
    "#     - 'stratum_attn': LSTM + LayerNorm + self-attention (single-head)\n",
    "#     - 'stratum_hydra': LSTM + LayerNorm + MultiHeadAttention + pooling\n",
    "\n",
    "MODEL_TYPE = \"stratum_hydra\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e47b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Cosine similarity loss function (optional)\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - K.sum(y_true * y_pred, axis=-1)\n",
    "\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse = MeanSquaredError()\n",
    "    return 0.7 * mse(y_true, y_pred) + 0.3 * cosine_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e264977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Loss selection logic\n",
    "if USE_HYBRID_LOSS:\n",
    "    loss_fn = hybrid_loss\n",
    "    loss_fn.__name__ = \"hybrid_loss\"\n",
    "elif USE_COSINE_LOSS:\n",
    "    loss_fn = cosine_loss\n",
    "    loss_fn.__name__ = \"cosine_loss\"\n",
    "else:\n",
    "    loss_fn = \"mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420b667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forecaster(input_shape, output_dim, big_mama=False, normalize_output=True):\n",
    "    model = Sequential()\n",
    "\n",
    "    if big_mama:\n",
    "        # ðŸ§  Big Bad Oh Mama Model\n",
    "        # model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "        # model.add(LSTM(64))\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape))\n",
    "        model.add(LSTM(64))  # Optional second LSTM unidirectional        \n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "    else:\n",
    "        # ðŸ§ƒ Simple Model\n",
    "        model.add(LSTM(64, input_shape=input_shape, return_sequences=False))\n",
    "\n",
    "    model.add(Dense(output_dim))\n",
    "\n",
    "    if normalize_output:\n",
    "        model.add(Lambda(lambda x: K.l2_normalize(x, axis=-1)))  # Normalize output\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283dd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecasterFactory:\n",
    "    \"\"\"\n",
    "    A flexible builder for LSTM-based forecasting architectures.\n",
    "\n",
    "    Supported model types:\n",
    "        - 'simple': Single-layer LSTM\n",
    "        - 'stratum': Stacked LSTM + dropout\n",
    "        - 'stratum_bi': BiLSTM + LSTM + dropout\n",
    "        - 'stratum_attn': LSTM + LayerNorm + self-attention (single-head)\n",
    "        - 'stratum_hydra': LSTM + LayerNorm + MultiHeadAttention + pooling\n",
    "\n",
    "    Parameters:\n",
    "        input_shape (tuple): Input shape of the time series (timesteps, features)\n",
    "        output_dim (int): Output dimensionality of the forecast embedding\n",
    "        normalize_output (bool): Whether to apply L2 normalization to output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, output_dim, normalize_output=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dim = output_dim\n",
    "        self.normalize_output = normalize_output\n",
    "\n",
    "    def build(self, model_type=\"simple\"):\n",
    "        # import tensorflow.keras.backend as K\n",
    "        # from tensorflow.keras.models import Sequential, Model\n",
    "        # from tensorflow.keras.layers import (\n",
    "        #     Input,\n",
    "        #     LSTM,\n",
    "        #     Dense,\n",
    "        #     Dropout,\n",
    "        #     Bidirectional,\n",
    "        #     Lambda,\n",
    "        #     LayerNormalization,\n",
    "        #     MultiHeadAttention,\n",
    "        #     GlobalAveragePooling1D,\n",
    "        # )\n",
    "\n",
    "        if model_type == \"stratum_hydra\":\n",
    "            # ðŸ§  Functional model for MHA\n",
    "            inputs = Input(shape=self.input_shape, name=\"input\")\n",
    "            x = LSTM(64, return_sequences=True)(inputs)\n",
    "            x = LayerNormalization()(x)\n",
    "            x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = Dense(self.output_dim)(x)\n",
    "\n",
    "            if self.normalize_output:\n",
    "                x = Lambda(lambda t: K.l2_normalize(t, axis=-1))(x)\n",
    "\n",
    "            return Model(inputs, x, name=\"stratum_hydra_forecaster\")\n",
    "\n",
    "        else:\n",
    "            # ðŸ§ƒ Sequential model for other types\n",
    "            model = Sequential(name=f\"{model_type}_forecaster\")\n",
    "\n",
    "            if model_type == \"simple\":\n",
    "                model.add(LSTM(64, input_shape=self.input_shape))\n",
    "\n",
    "            elif model_type == \"stratum\":\n",
    "                model.add(LSTM(64, return_sequences=True, input_shape=self.input_shape))\n",
    "                model.add(LSTM(64))\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "            elif model_type == \"stratum_bi\":\n",
    "                model.add(\n",
    "                    Bidirectional(\n",
    "                        LSTM(64, return_sequences=True), input_shape=self.input_shape\n",
    "                    )\n",
    "                )\n",
    "                model.add(LSTM(64))\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "            elif model_type == \"stratum_attn\":\n",
    "                model.add(\n",
    "                    Bidirectional(\n",
    "                        LSTM(64, return_sequences=True), input_shape=self.input_shape\n",
    "                    )\n",
    "                )\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LayerNormalization())\n",
    "                model.add(Dense(1, activation=\"tanh\"))  # attention scores\n",
    "                model.add(Lambda(lambda x: K.softmax(x, axis=1)))  # attention weights\n",
    "                model.add(\n",
    "                    Lambda(lambda x: K.sum(x * x, axis=1))\n",
    "                )  # weighted sum (fallback)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "            model.add(Dense(self.output_dim))\n",
    "\n",
    "            if self.normalize_output:\n",
    "                model.add(Lambda(lambda x: K.l2_normalize(x, axis=-1)))\n",
    "\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c1f36df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# model = build_forecaster(\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#     input_shape=input_shape,\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     output_dim=output_dim,\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#     big_mama=USE_BIG_MAMA_MODEL,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#     normalize_output=USE_OUTPUT_NORMALIZATION,\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     17\u001b[39m factory = ForecasterFactory(input_shape=input_shape, output_dim=output_dim)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_TYPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m model.summary()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# ðŸ§ª Compile\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mForecasterFactory.build\u001b[39m\u001b[34m(self, model_type)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.normalize_output:\n\u001b[32m     48\u001b[39m         x = Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m t: K.l2_normalize(t, axis=-\u001b[32m1\u001b[39m))(x)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m(inputs, x, name=\u001b[33m\"\u001b[39m\u001b[33mstratum_hydra_forecaster\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# ðŸ§ƒ Sequential model for other types\u001b[39;00m\n\u001b[32m     54\u001b[39m     model = Sequential(name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_forecaster\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "# âš ï¸ Adjust if you did Step 3:\n",
    "# X_train, y_train, X_val, y_val must be defined if you did a split.\n",
    "# Otherwise use full dataset:\n",
    "X_train, y_train = X, y\n",
    "X_val, y_val = None, None\n",
    "\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "# model = build_forecaster(\n",
    "#     input_shape=input_shape,\n",
    "#     output_dim=output_dim,\n",
    "#     big_mama=USE_BIG_MAMA_MODEL,\n",
    "#     normalize_output=USE_OUTPUT_NORMALIZATION,\n",
    "# )\n",
    "\n",
    "factory = ForecasterFactory(input_shape=input_shape, output_dim=output_dim)\n",
    "model = factory.build(model_type=MODEL_TYPE)\n",
    "model.summary()\n",
    "\n",
    "# ðŸ§ª Compile\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965393b3",
   "metadata": {},
   "source": [
    "### **Step 5: Train the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_callbacks(use_validation=True, early_stop_patience=PATIENCE):\n",
    "    monitor_metric = \"val_loss\" if use_validation else \"loss\"\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=monitor_metric,\n",
    "        patience=early_stop_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        monitor=monitor_metric, factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stop, lr_scheduler] if use_validation else [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = build_training_callbacks(USE_VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.2 if USE_VALIDATION else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=validation_split,\n",
    "    validation_data=(X_val, y_val) if X_val is not None else None,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,        # Important for time series\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063737a",
   "metadata": {},
   "source": [
    "### **Step 6: Predict Next Embedding**\n",
    "\n",
    "* Pick a time slice (e.g. last 5 embeddings)\n",
    "* Run `model.predict()` to get `EÌ‚[t+1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a03b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_sequence = embeddings[-window_size:]  # shape: (5, 64)\n",
    "X_input = np.expand_dims(recent_sequence, axis=0)  # shape: (1, 5, 64)\n",
    "E_hat = model.predict(X_input)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c852",
   "metadata": {},
   "source": [
    "### **Step 7: Assign Cluster to Predicted Embedding**\n",
    "\n",
    "* Scale `EÌ‚[t+1]` using `StandardScaler` (same one from original clustering)\n",
    "* Pass into `KNN.predict([EÌ‚_scaled])` to get `Cluster_ID[t+1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d2290",
   "metadata": {},
   "source": [
    "- Recreate the Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Recreate the scaler using original embeddings\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = embeddings \n",
    "\n",
    "print(\"âœ… Scaler fitted to original embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bcf8f",
   "metadata": {},
   "source": [
    "- Recreate and Save (optional) the assigned spectral_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV with headers\n",
    "df_clusters = pd.read_csv(cluster_path)\n",
    "\n",
    "# Extract the cluster labels\n",
    "cluster_labels = df_clusters[\"Cluster_ID\"].dropna().values\n",
    "\n",
    "# âœ… Confirm shape and preview\n",
    "print(f\"âœ… Loaded {len(cluster_labels)} cluster assignments.\")\n",
    "print(\"Clusters:\", np.unique(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98602aff",
   "metadata": {},
   "source": [
    "- Check integrity of cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for NaNs\n",
    "num_nans = np.isnan(cluster_labels).sum()\n",
    "assert num_nans == 0, f\"âŒ Cluster labels contain {num_nans} NaN values.\"\n",
    "\n",
    "# 2. Check length match with embeddings\n",
    "assert (\n",
    "    len(cluster_labels) == embeddings.shape[0]\n",
    "), f\"âŒ Length mismatch: {len(cluster_labels)} cluster labels vs {embeddings.shape[0]} embeddings.\"\n",
    "\n",
    "print(\"âœ… Cluster assignment checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff409e",
   "metadata": {},
   "source": [
    "#### ðŸ§  Train KNN on scaled embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ef1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Train KNN on scaled embeddings\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(embeddings_scaled, cluster_labels)\n",
    "\n",
    "# ðŸ’¾ (Optional) Save models\n",
    "joblib.dump(scaler, \"scaler_for_embeddings.pkl\")\n",
    "joblib.dump(knn, \"knn_predictor_on_spectral.pkl\")\n",
    "\n",
    "print(\"âœ… KNN predictor and scaler trained on cleaned data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68f998",
   "metadata": {},
   "source": [
    "### **Step 8: Evaluate (Optional)**\n",
    "\n",
    "* Compare predicted cluster with actual `Cluster_ID[t+1]`\n",
    "* Compute:\n",
    "\n",
    "  * Accuracy\n",
    "  * Confusion matrix\n",
    "  * Embedding prediction error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_metrics(df_perf, k=3):\n",
    "    df_sorted = df_perf.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "    best = df_sorted.head(k)\n",
    "    worst = df_sorted[df_sorted[\"F1\"] > 0].tail(k)  # exclude 0-F1 (like cluster 5)\n",
    "\n",
    "    print(f\"\\nâœ… Top {k} Clusters by F1 Score:\")\n",
    "    for _, row in best.iterrows():\n",
    "        print(\n",
    "            f\"  â€¢ Cluster {int(row['Cluster'])}: F1 = {row['F1']:.3f} (Precision = {row['Precision']:.3f}, Recall = {row['Recall']:.3f})\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nâŒ Bottom {k} Clusters by F1 Score (non-zero only):\")\n",
    "    for _, row in worst.iterrows():\n",
    "        print(\n",
    "            f\"  â€¢ Cluster {int(row['Cluster'])}: F1 = {row['F1']:.3f} (Precision = {row['Precision']:.3f}, Recall = {row['Recall']:.3f})\"\n",
    "        )\n",
    "\n",
    "    zero_f1 = df_perf[df_perf[\"F1\"] == 0]\n",
    "    if not zero_f1.empty:\n",
    "        print(f\"\\nðŸ’€ Clusters with F1 = 0:\")\n",
    "        for _, row in zero_f1.iterrows():\n",
    "            print(\n",
    "                f\"  â€¢ Cluster {int(row['Cluster'])}: (Precision = {row['Precision']:.3f}, Recall = {row['Recall']:.3f})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ Adjust for alignment: cluster_labels[window_size:] corresponds to y\n",
    "true_clusters = cluster_labels[window_size:]  # Shifted to match y[i] = E[t+window_size]\n",
    "\n",
    "# ðŸ”® Predict next embedding for all X\n",
    "predicted_embeddings = model.predict(X)  # shape: (num_samples, embedding_dim)\n",
    "print(f'Final Norms: {np.round(np.linalg.norm(predicted_embeddings, axis=1).mean(), 4)}')\n",
    "\n",
    "# ðŸ” Scale predicted embeddings\n",
    "predicted_embeddings_scaled = predicted_embeddings #scaler.transform(predicted_embeddings)\n",
    "\n",
    "# ðŸ§  Predict clusters\n",
    "predicted_clusters = knn.predict(predicted_embeddings_scaled)\n",
    "\n",
    "# ðŸ§ª Accuracy\n",
    "acc = accuracy_score(true_clusters, predicted_clusters)\n",
    "print(f\"âœ… Cluster prediction accuracy: {acc:.4f}\")\n",
    "\n",
    "# ðŸ§® Confusion matrix\n",
    "cm = confusion_matrix(true_clusters, predicted_clusters)\n",
    "print(\"ðŸ” Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3db31b",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Visualize the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5460252",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Cluster\")\n",
    "plt.ylabel(\"True Cluster\")\n",
    "plt.title(\"Confusion Matrix â€” Predicted vs True Cluster_ID[t+1]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9da4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_per_cluster_accuracy(cm):\n",
    "    \"\"\"\n",
    "    Print per-cluster accuracy from a given confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        cm (np.ndarray): Square confusion matrix where rows = true labels, cols = predicted labels.\n",
    "    \"\"\"\n",
    "    per_cluster_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\nðŸŽ¯ Per-Cluster Accuracy:\")\n",
    "    for i, acc in enumerate(per_cluster_acc):\n",
    "        print(f\"  â€¢ Cluster {i}: Accuracy = {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ea506",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    true_clusters, predicted_clusters, average=None, zero_division=0\n",
    ")\n",
    "df_perf = pd.DataFrame(\n",
    "    {\n",
    "        \"Cluster\": list(range(len(precision))),\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cluster_distribution(cluster_labels, window_size, val_pct=0.2):\n",
    "    # Align cluster labels to match y[t] (since y starts after window_size)\n",
    "    cluster_labels = cluster_labels[window_size:]\n",
    "    total_len = len(cluster_labels)\n",
    "    split_idx = int((1 - val_pct) * total_len)\n",
    "\n",
    "    train_clusters = cluster_labels[:split_idx]\n",
    "    val_clusters = cluster_labels[split_idx:]\n",
    "\n",
    "    # Count unique clusters\n",
    "    train_counts = Counter(train_clusters)\n",
    "    val_counts = Counter(val_clusters)\n",
    "    all_clusters = sorted(set(cluster_labels[~np.isnan(cluster_labels)]))\n",
    "\n",
    "    print(\"ðŸ“Š Per-Cluster Sample Counts:\")\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Cluster\": all_clusters,\n",
    "            \"Train Count\": [train_counts.get(c, 0) for c in all_clusters],\n",
    "            \"Val Count\": [val_counts.get(c, 0) for c in all_clusters],\n",
    "            \"Total Count\": [\n",
    "                train_counts.get(c, 0) + val_counts.get(c, 0) for c in all_clusters\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # ðŸ”” Warn about missing clusters in training\n",
    "    missing = [c for c in all_clusters if train_counts.get(c, 0) == 0]\n",
    "    if missing:\n",
    "        print(f\"\\nðŸš¨ Warning: Clusters missing in training set: {missing}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… All clusters present in training set.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_training_run(config, cm, df_perf):\n",
    "    \"\"\"\n",
    "    Print a full summary including config, per-cluster accuracy, and F1 breakdown.\n",
    "\n",
    "    Args:\n",
    "        config (dict): {\n",
    "            \"Model Type\": ..., \"Loss\": ..., \"Epochs\": ..., etc.\n",
    "        }\n",
    "        cm (np.ndarray): Confusion matrix.\n",
    "        df_perf (pd.DataFrame): Precision, Recall, F1 per cluster.\n",
    "    \"\"\"\n",
    "    # Header\n",
    "    print(\"âš™ï¸ Training Summary:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\nðŸ” Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    print_per_cluster_accuracy(cm)\n",
    "\n",
    "    # F1 summary\n",
    "    print_summary_metrics(df_perf)\n",
    "    \n",
    "    check_cluster_distribution(cluster_labels, window_size=window_size, val_pct=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc6b6a",
   "metadata": {},
   "source": [
    "### âš™ï¸ Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§® Extract metrics from history\n",
    "epochs_run = len(history.history[\"loss\"])\n",
    "final_train_loss = history.history[\"loss\"][-1]\n",
    "final_val_loss = history.history[\"val_loss\"][-1] if USE_VALIDATION else None\n",
    "\n",
    "# ðŸ§  Format labels\n",
    "loss_name = loss_fn.__name__ if callable(loss_fn) else str(loss_fn)\n",
    "callback_names = (\n",
    "    [cb.__class__.__name__ for cb in callbacks] if \"callbacks\" in locals() else []\n",
    ")\n",
    "model_type = model.name if hasattr(model, \"name\") else \"Unknown\"\n",
    "val_pct = f\"{int(validation_split * 100)}%\" if validation_split > 0 else \"None\"\n",
    "\n",
    "# ðŸ“¦ Build config dict\n",
    "config = {\n",
    "    \"Model Type\": model_type,\n",
    "    \"Output Normalization\": \"Enabled\" if USE_OUTPUT_NORMALIZATION else \"Disabled\",\n",
    "    \"Loss Function\": loss_name,\n",
    "    \"Validation Split\": val_pct,\n",
    "    \"Callbacks Used\": \", \".join(callback_names) if callback_names else \"None\",\n",
    "    \"Epochs Run\": epochs_run,\n",
    "    \"Final Train Loss\": f\"{final_train_loss:.6f}\",\n",
    "}\n",
    "if final_val_loss is not None:\n",
    "    config[\"Final Val Loss\"] = f\"{final_val_loss:.6f}\"\n",
    "\n",
    "# ðŸ“Š Final report\n",
    "summarize_training_run(config, cm, df_perf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
