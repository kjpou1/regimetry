{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841a8fa1",
   "metadata": {},
   "source": [
    "## ğŸ§­ GOAL:\n",
    "Train a self-supervised model to forecast E[t+1] from [E[t-n], ..., E[t]], then map the forecasted embedding to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6678c38",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## âœ… STEP-BY-STEP OUTLINE\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Load Embeddings**\n",
    "\n",
    "* Load `embeddings.csv` into a NumPy array or pandas DataFrame.\n",
    "* Confirm shape: `(num_timesteps, embedding_dim)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Build Sliding Window Dataset**\n",
    "\n",
    "Create training samples:\n",
    "\n",
    "* Input: `X[i] = [E[i], E[i+1], ..., E[i+window_size-1]]`\n",
    "* Target: `y[i] = E[i+window_size]`\n",
    "\n",
    "This results in:\n",
    "\n",
    "* `X`: shape `(num_samples, window_size, embedding_dim)`\n",
    "* `y`: shape `(num_samples, embedding_dim)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Train/Test Split (Optional)**\n",
    "\n",
    "* Use 80/20 split (or time-ordered split) for validation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Define LSTM Model in Keras**\n",
    "\n",
    "* Use `Sequential` API:\n",
    "\n",
    "  * LSTM layer â†’ Dense output\n",
    "* Compile with:\n",
    "\n",
    "  * Loss: `'mse'`\n",
    "  * Optimizer: `'adam'`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Train the Model**\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=..., batch_size=...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Predict Next Embedding**\n",
    "\n",
    "* Pick a time slice (e.g. last 5 embeddings)\n",
    "* Run `model.predict()` to get `EÌ‚[t+1]`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Assign Cluster to Predicted Embedding**\n",
    "\n",
    "* Scale `EÌ‚[t+1]` using `StandardScaler` (same one from original clustering)\n",
    "* Pass into `KNN.predict([EÌ‚_scaled])` to get `Cluster_ID[t+1]`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Evaluate (Optional)**\n",
    "\n",
    "* Compare predicted cluster with actual `Cluster_ID[t+1]`\n",
    "* Compute:\n",
    "\n",
    "  * Accuracy\n",
    "  * Confusion matrix\n",
    "  * Embedding prediction error (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cb4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    "    Lambda,\n",
    "    Bidirectional,\n",
    "    Input,\n",
    "    MultiHeadAttention,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from regimetry.config import Config\n",
    "from regimetry.logger_manager import LoggerManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ac07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = LoggerManager.get_logger(\"unsupervised_next_cluster_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cf7d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT ROOT: /Users/kenneth/Public/projects/python/ai/regimetry\n",
      "BASE DIR: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts\n",
      "RAW DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/data/raw\n",
      "PROCESSED DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/data/processed\n",
      "EMBEDDINGS DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/embeddings\n",
      "REPORTS DATA: /Users/kenneth/Public/projects/python/ai/regimetry/artifacts/reports\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "print(\"PROJECT ROOT:\", cfg.PROJECT_ROOT)\n",
    "print(\"BASE DIR:\", cfg.BASE_DIR)\n",
    "print(\"RAW DATA:\", cfg.RAW_DATA_DIR)\n",
    "print(\"PROCESSED DATA:\", cfg.PROCESSED_DATA_DIR)\n",
    "print(\"EMBEDDINGS DATA:\", cfg.EMBEDDINGS_DIR) \n",
    "print(\"REPORTS DATA:\", cfg.REPORTS_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9fc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = path.join(cfg.EMBEDDINGS_DIR, \"GBP_USD/ws5/learnable64_default\")\n",
    "cluster_path = path.join(\n",
    "    cfg.REPORTS_DIR, \"GBP_USD/ws5/learnable64/default/nc8\", \"cluster_assignments.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffd90c",
   "metadata": {},
   "source": [
    "### **Step 1: Load Embeddings**\n",
    "\n",
    "* Load `embeddings.csv` into a NumPy array or pandas DataFrame.\n",
    "* Confirm shape: `(num_timesteps, embedding_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43fc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded embeddings with shape: (1881, 64)\n"
     ]
    }
   ],
   "source": [
    "embedding_file = path.join(\n",
    "    embedding_path,\n",
    "    \"embedding.npy\",\n",
    ")\n",
    "# ğŸ§¾ Load as NumPy array\n",
    "embeddings = np.load(embedding_file)\n",
    "\n",
    "# âœ… Confirm shape\n",
    "num_timesteps, embedding_dim = embeddings.shape\n",
    "print(f\"âœ… Loaded embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a001b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = path.join(\n",
    "    embedding_path,\n",
    "    \"embedding_metadata.json\",\n",
    ")\n",
    "\n",
    "with open(metadata_file) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# print(json.dumps(metadata, indent=2))\n",
    "assert embeddings.shape == (metadata[\"n_samples\"], metadata[\"embedding_dim\"])\n",
    "# ğŸ”§ Set window size\n",
    "window_size = metadata[\"window_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ebb7b",
   "metadata": {},
   "source": [
    "### **Step 2: Build Sliding Window Dataset**\n",
    "\n",
    "Create training samples:\n",
    "\n",
    "* Input: `X[i] = [E[i], E[i+1], ..., E[i+window_size-1]]`\n",
    "* Target: `y[i] = E[i+window_size]`\n",
    "\n",
    "This results in:\n",
    "\n",
    "* `X`: shape `(num_samples, window_size, embedding_dim)`\n",
    "* `y`: shape `(num_samples, embedding_dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5ac7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1876, 5, 64)  (samples, window_size, embedding_dim)\n",
      "y shape: (1876, 64)  (samples, embedding_dim)\n"
     ]
    }
   ],
   "source": [
    "embeddings = normalize(embeddings, norm=\"l2\", axis=1)\n",
    "\n",
    "def build_embedding_forecast_dataset(embeddings, window_size=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(embeddings) - window_size):\n",
    "        X.append(embeddings[i : i + window_size])\n",
    "        y.append(embeddings[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# âš™ï¸ Build dataset\n",
    "X, y = build_embedding_forecast_dataset(embeddings, window_size)\n",
    "\n",
    "# âœ… Confirm shape\n",
    "print(f\"X shape: {X.shape}  (samples, window_size, embedding_dim)\")\n",
    "print(f\"y shape: {y.shape}  (samples, embedding_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbe608",
   "metadata": {},
   "source": [
    "#### Integrity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8f8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All dataset integrity checks passed.\n"
     ]
    }
   ],
   "source": [
    "# From metadata\n",
    "expected_window_size = metadata[\"window_size\"]\n",
    "expected_embedding_dim = metadata[\"embedding_dim\"]\n",
    "expected_num_samples = metadata[\"n_samples\"]\n",
    "\n",
    "# 1. Validate embedding dimensions\n",
    "assert embeddings.shape == (\n",
    "    expected_num_samples,\n",
    "    expected_embedding_dim,\n",
    "), f\"âŒ Embeddings shape mismatch: expected ({expected_num_samples}, {expected_embedding_dim}), got {embeddings.shape}\"\n",
    "\n",
    "# 2. Validate window size\n",
    "assert (\n",
    "    X.shape[1] == expected_window_size\n",
    "), f\"âŒ Window size mismatch: expected {expected_window_size}, got {X.shape[1]}\"\n",
    "\n",
    "# 3. Validate embedding dimension in X and y\n",
    "assert (\n",
    "    X.shape[2] == expected_embedding_dim\n",
    "), f\"âŒ Embedding dim in X mismatch: expected {expected_embedding_dim}, got {X.shape[2]}\"\n",
    "assert (\n",
    "    y.shape[1] == expected_embedding_dim\n",
    "), f\"âŒ Embedding dim in y mismatch: expected {expected_embedding_dim}, got {y.shape[1]}\"\n",
    "\n",
    "# 4. Sanity check on total samples\n",
    "expected_num_sequences = expected_num_samples - expected_window_size\n",
    "assert (\n",
    "    X.shape[0] == expected_num_sequences == y.shape[0]\n",
    "), f\"âŒ Sequence count mismatch: expected {expected_num_sequences}, got X: {X.shape[0]}, y: {y.shape[0]}\"\n",
    "\n",
    "print(\"âœ… All dataset integrity checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306720",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 3 (Optional): Chronological Train/Validation Split\n",
    "\n",
    "### ğŸ¯ Purpose\n",
    "\n",
    "Split your time-series embedding dataset into:\n",
    "\n",
    "* `train` set: to learn embedding dynamics\n",
    "* `validation` set: to **monitor overfitting**, **tune architecture**, and **evaluate generalization**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Do It\n",
    "\n",
    "* You want to track model performance realistically\n",
    "* You plan to compare models or tune hyperparameters\n",
    "* You're preparing the model for production use or scientific evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš« When It's Safe to Skip (Temporarily)\n",
    "\n",
    "* You just want to test if the LSTM can **produce a meaningful `EÌ‚[t+1]`**\n",
    "* Youâ€™re validating the **cluster assignment logic**, not the embedding quality\n",
    "* Youâ€™ll circle back later to do formal evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617146ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a005a",
   "metadata": {},
   "source": [
    "### **Step 4: Define LSTM Model in Keras**\n",
    "to predict `E[t+1]` from the previous `window_size` embeddings.\n",
    "\n",
    "* Use `Sequential` API:\n",
    "\n",
    "  * LSTM layer â†’ Dense output\n",
    "* Compile with:\n",
    "\n",
    "  * Loss: `'mse'`\n",
    "  * Optimizer: `'adam'`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482229a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ” What This Does\n",
    "\n",
    "| Layer              | Purpose                                     |\n",
    "| ------------------ | ------------------------------------------- |\n",
    "| `LSTM(64)`         | Learns sequence pattern in embedding flow   |\n",
    "| `Dense(64)`        | Outputs the next predicted embedding vector |\n",
    "| Loss = `MSE`       | Because embeddings are continuous vectors   |\n",
    "| Optimizer = `Adam` | Well-suited for smooth loss landscapes      |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Result\n",
    "\n",
    "You now have a trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31192889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Choose loss function\n",
    "USE_COSINE_LOSS = True\n",
    "USE_HYBRID_LOSS = False  # Toggle this\n",
    "USE_VALIDATION = False  # Toggle this\n",
    "USE_BIG_MAMA_MODEL = True  # ğŸ” Toggle this to switch architectures\n",
    "USE_OUTPUT_NORMALIZATION = True  # Controls whether final output is L2-normalized\n",
    "EPOCHS = 300  # bump it up\n",
    "PATIENCE = 20  # give EarlyStopping some breathing room\n",
    "\n",
    "# Supported model types:\n",
    "#     - 'simple': Single-layer LSTM\n",
    "#     - 'stratum': Stacked LSTM + dropout\n",
    "#     - 'stratum_bi': BiLSTM + LSTM + dropout\n",
    "#     - 'stratum_attn': LSTM + LayerNorm + self-attention (single-head)\n",
    "#     - 'stratum_hydra': LSTM + LayerNorm + MultiHeadAttention + pooling\n",
    "\n",
    "MODEL_TYPE = \"stratum_hydra\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e47b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Cosine similarity loss function (optional)\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - K.sum(y_true * y_pred, axis=-1)\n",
    "\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse = MeanSquaredError()\n",
    "    return 0.7 * mse(y_true, y_pred) + 0.3 * cosine_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e264977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ‘‡ Loss selection logic\n",
    "if USE_HYBRID_LOSS:\n",
    "    loss_fn = hybrid_loss\n",
    "    loss_fn.__name__ = \"hybrid_loss\"\n",
    "elif USE_COSINE_LOSS:\n",
    "    loss_fn = cosine_loss\n",
    "    loss_fn.__name__ = \"cosine_loss\"\n",
    "else:\n",
    "    loss_fn = \"mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "283dd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecasterFactory:\n",
    "    \"\"\"\n",
    "    A flexible builder for LSTM-based forecasting architectures.\n",
    "\n",
    "    Supported model types:\n",
    "        - 'simple': Single-layer LSTM\n",
    "        - 'stratum': Stacked LSTM + dropout\n",
    "        - 'stratum_bi': BiLSTM + LSTM + dropout\n",
    "        - 'stratum_attn': LSTM + LayerNorm + self-attention (single-head)\n",
    "        - 'stratum_hydra': LSTM + LayerNorm + MultiHeadAttention + pooling\n",
    "\n",
    "    Parameters:\n",
    "        input_shape (tuple): Input shape of the time series (timesteps, features)\n",
    "        output_dim (int): Output dimensionality of the forecast embedding\n",
    "        normalize_output (bool): Whether to apply L2 normalization to output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, output_dim, normalize_output=True):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dim = output_dim\n",
    "        self.normalize_output = normalize_output\n",
    "\n",
    "    def build(self, model_type=\"simple\"):\n",
    "\n",
    "        if model_type == \"stratum_hydra\":\n",
    "            # ğŸ§  Functional model for MHA\n",
    "            inputs = Input(shape=self.input_shape, name=\"input\")\n",
    "            x = LSTM(64, return_sequences=True)(inputs)\n",
    "            x = LayerNormalization()(x)\n",
    "            x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = Dense(self.output_dim)(x)\n",
    "\n",
    "            if self.normalize_output:\n",
    "                x = Lambda(lambda t: K.l2_normalize(t, axis=-1))(x)\n",
    "\n",
    "            return Model(inputs, x, name=\"stratum_hydra_forecaster\")\n",
    "\n",
    "        else:\n",
    "            # ğŸ§ƒ Sequential model for other types\n",
    "            model = Sequential(name=f\"{model_type}_forecaster\")\n",
    "\n",
    "            if model_type == \"simple\":\n",
    "                model.add(LSTM(64, input_shape=self.input_shape))\n",
    "\n",
    "            elif model_type == \"stratum\":\n",
    "                model.add(LSTM(64, return_sequences=True, input_shape=self.input_shape))\n",
    "                model.add(LSTM(64))\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "            elif model_type == \"stratum_bi\":\n",
    "                model.add(\n",
    "                    Bidirectional(\n",
    "                        LSTM(64, return_sequences=True), input_shape=self.input_shape\n",
    "                    )\n",
    "                )\n",
    "                model.add(LSTM(64))\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "            elif model_type == \"stratum_attn\":\n",
    "                model.add(\n",
    "                    Bidirectional(\n",
    "                        LSTM(64, return_sequences=True), input_shape=self.input_shape\n",
    "                    )\n",
    "                )\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LayerNormalization())\n",
    "                model.add(Dense(1, activation=\"tanh\"))  # attention scores\n",
    "                model.add(Lambda(lambda x: K.softmax(x, axis=1)))  # attention weights\n",
    "                model.add(\n",
    "                    Lambda(lambda x: K.sum(x * x, axis=1))\n",
    "                )  # weighted sum (fallback)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "            model.add(Dense(self.output_dim))\n",
    "\n",
    "            if self.normalize_output:\n",
    "                model.add(Lambda(lambda x: K.l2_normalize(x, axis=-1)))\n",
    "\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1f36df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"stratum_hydra_forecaster\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"stratum_hydra_forecaster\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_head_attentiâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,216</span> â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentioâ€¦</span> â”‚                   â”‚            â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_head_attenâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (\u001b[38;5;33mInputLayer\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚     \u001b[38;5;34m33,024\u001b[0m â”‚ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m128\u001b[0m â”‚ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_head_attentiâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚     \u001b[38;5;34m33,216\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadAttentioâ€¦\u001b[0m â”‚                   â”‚            â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ multi_head_attenâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m4,160\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lambda (\u001b[38;5;33mLambda\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,528</span> (275.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,528\u001b[0m (275.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,528</span> (275.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,528\u001b[0m (275.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âš ï¸ Adjust if you did Step 3:\n",
    "# X_train, y_train, X_val, y_val must be defined if you did a split.\n",
    "# Otherwise use full dataset:\n",
    "X_train, y_train = X, y\n",
    "X_val, y_val = None, None\n",
    "\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "factory = ForecasterFactory(input_shape=input_shape, output_dim=output_dim)\n",
    "model = factory.build(model_type=MODEL_TYPE)\n",
    "model.summary()\n",
    "\n",
    "# ğŸ§ª Compile\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965393b3",
   "metadata": {},
   "source": [
    "### **Step 5: Train the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c092b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_callbacks(use_validation=True, early_stop_patience=PATIENCE):\n",
    "    monitor_metric = \"val_loss\" if use_validation else \"loss\"\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=monitor_metric,\n",
    "        patience=early_stop_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        monitor=monitor_metric, factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stop, lr_scheduler] if use_validation else [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b5d9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = build_training_callbacks(USE_VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f10b6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.2 if USE_VALIDATION else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b573c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5174 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2561 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1676 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1330 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1100 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0926 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0805 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0729 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0677 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0632 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0594 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0563 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0536 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0514 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0518 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0531 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0501 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0495 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0460 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0442 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0405 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0402 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0390 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0388 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0378 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0373 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0365 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0367 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0363 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0358 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0352 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0350 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0351 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0356 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0360 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0340 - learning_rate: 0.0010\n",
      "Epoch 41/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0358 - learning_rate: 0.0010\n",
      "Epoch 42/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0361 - learning_rate: 0.0010\n",
      "Epoch 43/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0354 - learning_rate: 0.0010\n",
      "Epoch 44/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0357 - learning_rate: 0.0010\n",
      "Epoch 45/300\n",
      "\u001b[1m46/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0374\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0367 - learning_rate: 0.0010\n",
      "Epoch 46/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0340 - learning_rate: 5.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0301 - learning_rate: 5.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0292 - learning_rate: 5.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0292 - learning_rate: 5.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0288 - learning_rate: 5.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0282 - learning_rate: 5.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0279 - learning_rate: 5.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0277 - learning_rate: 5.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0276 - learning_rate: 5.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0275 - learning_rate: 5.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0273 - learning_rate: 5.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0272 - learning_rate: 5.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0270 - learning_rate: 5.0000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0269 - learning_rate: 5.0000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0268 - learning_rate: 5.0000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0267 - learning_rate: 5.0000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0266 - learning_rate: 5.0000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0265 - learning_rate: 5.0000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0264 - learning_rate: 5.0000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0263 - learning_rate: 5.0000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0262 - learning_rate: 5.0000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0261 - learning_rate: 5.0000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0260 - learning_rate: 5.0000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0259 - learning_rate: 5.0000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0258 - learning_rate: 5.0000e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0257 - learning_rate: 5.0000e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0257 - learning_rate: 5.0000e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0256 - learning_rate: 5.0000e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0255 - learning_rate: 5.0000e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0254 - learning_rate: 5.0000e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0254 - learning_rate: 5.0000e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0253 - learning_rate: 5.0000e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0253 - learning_rate: 5.0000e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0253 - learning_rate: 5.0000e-04\n",
      "Epoch 80/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0254 - learning_rate: 5.0000e-04\n",
      "Epoch 81/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0258\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0258 - learning_rate: 5.0000e-04\n",
      "Epoch 82/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0254 - learning_rate: 2.5000e-04\n",
      "Epoch 83/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0253 - learning_rate: 2.5000e-04\n",
      "Epoch 84/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0250 - learning_rate: 2.5000e-04\n",
      "Epoch 85/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0251 - learning_rate: 2.5000e-04\n",
      "Epoch 86/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0249 - learning_rate: 2.5000e-04\n",
      "Epoch 87/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0244 - learning_rate: 2.5000e-04\n",
      "Epoch 88/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0241 - learning_rate: 2.5000e-04\n",
      "Epoch 89/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0240 - learning_rate: 2.5000e-04\n",
      "Epoch 90/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0239 - learning_rate: 2.5000e-04\n",
      "Epoch 91/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0238 - learning_rate: 2.5000e-04\n",
      "Epoch 92/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0238 - learning_rate: 2.5000e-04\n",
      "Epoch 93/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0237 - learning_rate: 2.5000e-04\n",
      "Epoch 94/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0236 - learning_rate: 2.5000e-04\n",
      "Epoch 95/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0236 - learning_rate: 2.5000e-04\n",
      "Epoch 96/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0235 - learning_rate: 2.5000e-04\n",
      "Epoch 97/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0235 - learning_rate: 2.5000e-04\n",
      "Epoch 98/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0234 - learning_rate: 2.5000e-04\n",
      "Epoch 99/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0233 - learning_rate: 2.5000e-04\n",
      "Epoch 100/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0233 - learning_rate: 2.5000e-04\n",
      "Epoch 101/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0233 - learning_rate: 2.5000e-04\n",
      "Epoch 102/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0232 - learning_rate: 2.5000e-04\n",
      "Epoch 103/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0232 - learning_rate: 2.5000e-04\n",
      "Epoch 104/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0231 - learning_rate: 2.5000e-04\n",
      "Epoch 105/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0231 - learning_rate: 2.5000e-04\n",
      "Epoch 106/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0230 - learning_rate: 2.5000e-04\n",
      "Epoch 107/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0230 - learning_rate: 2.5000e-04\n",
      "Epoch 108/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0229 - learning_rate: 2.5000e-04\n",
      "Epoch 109/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0229 - learning_rate: 2.5000e-04\n",
      "Epoch 110/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0229 - learning_rate: 2.5000e-04\n",
      "Epoch 111/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0228 - learning_rate: 2.5000e-04\n",
      "Epoch 112/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0228 - learning_rate: 2.5000e-04\n",
      "Epoch 113/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0227 - learning_rate: 2.5000e-04\n",
      "Epoch 114/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0227 - learning_rate: 2.5000e-04\n",
      "Epoch 115/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0226 - learning_rate: 2.5000e-04\n",
      "Epoch 116/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0226 - learning_rate: 2.5000e-04\n",
      "Epoch 117/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0226 - learning_rate: 2.5000e-04\n",
      "Epoch 118/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0225 - learning_rate: 2.5000e-04\n",
      "Epoch 119/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0225 - learning_rate: 2.5000e-04\n",
      "Epoch 120/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0224 - learning_rate: 2.5000e-04\n",
      "Epoch 121/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0224 - learning_rate: 2.5000e-04\n",
      "Epoch 122/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0224 - learning_rate: 2.5000e-04\n",
      "Epoch 123/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0223 - learning_rate: 2.5000e-04\n",
      "Epoch 124/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0223 - learning_rate: 2.5000e-04\n",
      "Epoch 125/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0223 - learning_rate: 2.5000e-04\n",
      "Epoch 126/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - learning_rate: 2.5000e-04\n",
      "Epoch 127/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - learning_rate: 2.5000e-04\n",
      "Epoch 128/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - learning_rate: 2.5000e-04\n",
      "Epoch 129/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0221 - learning_rate: 2.5000e-04\n",
      "Epoch 130/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0221 - learning_rate: 2.5000e-04\n",
      "Epoch 131/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - learning_rate: 2.5000e-04\n",
      "Epoch 132/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - learning_rate: 2.5000e-04\n",
      "Epoch 133/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0226 - learning_rate: 2.5000e-04\n",
      "Epoch 134/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0228 - learning_rate: 2.5000e-04\n",
      "Epoch 135/300\n",
      "\u001b[1m46/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0239\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0236 - learning_rate: 2.5000e-04\n",
      "Epoch 136/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0228 - learning_rate: 1.2500e-04\n",
      "Epoch 137/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0221 - learning_rate: 1.2500e-04\n",
      "Epoch 138/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0219 - learning_rate: 1.2500e-04\n",
      "Epoch 139/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0218 - learning_rate: 1.2500e-04\n",
      "Epoch 140/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0217 - learning_rate: 1.2500e-04\n",
      "Epoch 141/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0216 - learning_rate: 1.2500e-04\n",
      "Epoch 142/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0215 - learning_rate: 1.2500e-04\n",
      "Epoch 143/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0215 - learning_rate: 1.2500e-04\n",
      "Epoch 144/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0214 - learning_rate: 1.2500e-04\n",
      "Epoch 145/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0214 - learning_rate: 1.2500e-04\n",
      "Epoch 146/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0214 - learning_rate: 1.2500e-04\n",
      "Epoch 147/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0213 - learning_rate: 1.2500e-04\n",
      "Epoch 148/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0213 - learning_rate: 1.2500e-04\n",
      "Epoch 149/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0213 - learning_rate: 1.2500e-04\n",
      "Epoch 150/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0212 - learning_rate: 1.2500e-04\n",
      "Epoch 151/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0212 - learning_rate: 1.2500e-04\n",
      "Epoch 152/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0212 - learning_rate: 1.2500e-04\n",
      "Epoch 153/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0212 - learning_rate: 1.2500e-04\n",
      "Epoch 154/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0211 - learning_rate: 1.2500e-04\n",
      "Epoch 155/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0211 - learning_rate: 1.2500e-04\n",
      "Epoch 156/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0211 - learning_rate: 1.2500e-04\n",
      "Epoch 157/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0211 - learning_rate: 1.2500e-04\n",
      "Epoch 158/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 - learning_rate: 1.2500e-04\n",
      "Epoch 159/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 - learning_rate: 1.2500e-04\n",
      "Epoch 160/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 - learning_rate: 1.2500e-04\n",
      "Epoch 161/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 - learning_rate: 1.2500e-04\n",
      "Epoch 162/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - learning_rate: 1.2500e-04\n",
      "Epoch 163/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - learning_rate: 1.2500e-04\n",
      "Epoch 164/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - learning_rate: 1.2500e-04\n",
      "Epoch 165/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - learning_rate: 1.2500e-04\n",
      "Epoch 166/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 1.2500e-04\n",
      "Epoch 167/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 1.2500e-04\n",
      "Epoch 168/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 1.2500e-04\n",
      "Epoch 169/300\n",
      "\u001b[1m45/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 1.2500e-04\n",
      "Epoch 170/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0207 - learning_rate: 6.2500e-05\n",
      "Epoch 171/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 6.2500e-05\n",
      "Epoch 172/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 6.2500e-05\n",
      "Epoch 173/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - learning_rate: 6.2500e-05\n",
      "Epoch 174/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - learning_rate: 6.2500e-05\n",
      "Epoch 175/300\n",
      "\u001b[1m46/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - learning_rate: 6.2500e-05\n",
      "Epoch 176/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0207 - learning_rate: 3.1250e-05\n",
      "Epoch 177/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0206 - learning_rate: 3.1250e-05\n",
      "Epoch 178/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0205 - learning_rate: 3.1250e-05\n",
      "Epoch 179/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0204 - learning_rate: 3.1250e-05\n",
      "Epoch 180/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0204 - learning_rate: 3.1250e-05\n",
      "Epoch 181/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0204 - learning_rate: 3.1250e-05\n",
      "Epoch 182/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0203 - learning_rate: 3.1250e-05\n",
      "Epoch 183/300\n",
      "\u001b[1m45/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0205\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0203 - learning_rate: 3.1250e-05\n",
      "Epoch 184/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0203 - learning_rate: 1.5625e-05\n",
      "Epoch 185/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 1.5625e-05\n",
      "Epoch 186/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 1.5625e-05\n",
      "Epoch 187/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 1.5625e-05\n",
      "Epoch 188/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 1.5625e-05\n",
      "Epoch 189/300\n",
      "\u001b[1m45/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0203\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 1.5625e-05\n",
      "Epoch 190/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 7.8125e-06\n",
      "Epoch 191/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - learning_rate: 7.8125e-06\n",
      "Epoch 192/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 7.8125e-06\n",
      "Epoch 193/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 7.8125e-06\n",
      "Epoch 194/300\n",
      "\u001b[1m58/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 7.8125e-06\n",
      "Epoch 195/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 3.9063e-06\n",
      "Epoch 196/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 3.9063e-06\n",
      "Epoch 197/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 3.9063e-06\n",
      "Epoch 198/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 3.9063e-06\n",
      "Epoch 199/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 3.9063e-06\n",
      "Epoch 200/300\n",
      "\u001b[1m45/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0203\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 3.9063e-06\n",
      "Epoch 201/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.9531e-06\n",
      "Epoch 202/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.9531e-06\n",
      "Epoch 203/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.9531e-06\n",
      "Epoch 204/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.9531e-06\n",
      "Epoch 205/300\n",
      "\u001b[1m45/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0203\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.9531e-06\n",
      "Epoch 206/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 207/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 208/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 209/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 210/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 211/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 212/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 213/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 214/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 215/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 216/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 217/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 218/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 219/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 220/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 221/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 222/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 223/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 224/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 225/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 226/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 227/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 228/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 229/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 230/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 231/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 232/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 233/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 234/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 235/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 236/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 237/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 238/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 239/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 240/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 241/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 242/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 243/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 244/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 245/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 246/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 247/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 248/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 249/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 250/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 251/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 252/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 253/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 254/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 255/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 256/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 257/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 258/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 259/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 260/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 261/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 262/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 263/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 264/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 265/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 266/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 267/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 268/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 269/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 270/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 271/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 272/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 273/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 274/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 275/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 276/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 277/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 278/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 279/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 280/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 281/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 282/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 283/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 284/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 285/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 286/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 287/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 288/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 289/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 290/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 291/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 292/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 293/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 294/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 295/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 296/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 297/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 298/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 299/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n",
      "Epoch 300/300\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0201 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=validation_split,\n",
    "    validation_data=(X_val, y_val) if X_val is not None else None,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,        # Important for time series\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063737a",
   "metadata": {},
   "source": [
    "### **Step 6: Predict Next Embedding**\n",
    "\n",
    "* Pick a time slice (e.g. last 5 embeddings)\n",
    "* Run `model.predict()` to get `EÌ‚[t+1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a03b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    }
   ],
   "source": [
    "recent_sequence = embeddings[-window_size:]  # shape: (5, 64)\n",
    "X_input = np.expand_dims(recent_sequence, axis=0)  # shape: (1, 5, 64)\n",
    "E_hat = model.predict(X_input)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c852",
   "metadata": {},
   "source": [
    "### **Step 7: Train KNN on Embeddings & Predict Next Cluster**\n",
    "\n",
    "* Pass predicted embedding `EÌ‚[t+1]` directly into `KNN.predict([EÌ‚])` to assign `Cluster_ID[t+1]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bcf8f",
   "metadata": {},
   "source": [
    "- Recreate and Save (optional) the assigned spectral_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2911fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1881 cluster assignments.\n",
      "Clusters: [0. 1. 2. 3. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV with headers\n",
    "df_clusters = pd.read_csv(cluster_path)\n",
    "\n",
    "# Extract the cluster labels\n",
    "cluster_labels = df_clusters[\"Cluster_ID\"].dropna().values\n",
    "\n",
    "# âœ… Confirm shape and preview\n",
    "print(f\"âœ… Loaded {len(cluster_labels)} cluster assignments.\")\n",
    "print(\"Clusters:\", np.unique(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98602aff",
   "metadata": {},
   "source": [
    "- Check integrity of cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1cb02b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cluster assignment checks passed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for NaNs\n",
    "num_nans = np.isnan(cluster_labels).sum()\n",
    "assert num_nans == 0, f\"âŒ Cluster labels contain {num_nans} NaN values.\"\n",
    "\n",
    "# 2. Check length match with embeddings\n",
    "assert (\n",
    "    len(cluster_labels) == embeddings.shape[0]\n",
    "), f\"âŒ Length mismatch: {len(cluster_labels)} cluster labels vs {embeddings.shape[0]} embeddings.\"\n",
    "\n",
    "print(\"âœ… Cluster assignment checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff409e",
   "metadata": {},
   "source": [
    "#### ğŸ§  Train KNN on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "913ef1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KNN predictor trained on cleaned data.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§  Train KNN on scaled embeddings\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(embeddings, cluster_labels)\n",
    "\n",
    "# ğŸ’¾ (Optional) Save models\n",
    "joblib.dump(knn, \"knn_predictor_on_spectral.pkl\")\n",
    "\n",
    "print(\"âœ… KNN predictor trained on cleaned data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd444363",
   "metadata": {},
   "source": [
    "- Predict Cluster for EÌ‚[t+1]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7d530c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_prob = knn.predict_proba([E_hat])  # shape: (1, n_clusters)\n",
    "predicted_cluster = knn.predict([E_hat])[0]\n",
    "predicted_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68f998",
   "metadata": {},
   "source": [
    "### **Step 8: Evaluate (Optional)**\n",
    "\n",
    "* Compare predicted cluster with actual `Cluster_ID[t+1]`\n",
    "* Compute:\n",
    "\n",
    "  * Accuracy\n",
    "  * Confusion matrix\n",
    "  * Embedding prediction error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e9b116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_metrics(df_perf, k=3):\n",
    "    df_sorted = df_perf.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "    best = df_sorted.head(k)\n",
    "    worst = df_sorted[df_sorted[\"F1\"] > 0].tail(k)  # exclude 0-F1 (like cluster 5)\n",
    "\n",
    "    print(f\"\\nâœ… Top {k} Clusters by F1 Score:\")\n",
    "    for _, row in best.iterrows():\n",
    "        print(\n",
    "            f\"  â€¢ Cluster {int(row['Cluster'])}: F1 = {row['F1']:.3f} (Precision = {row['Precision']:.3f}, Recall = {row['Recall']:.3f})\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nâŒ Bottom {k} Clusters by F1 Score (non-zero only):\")\n",
    "    for _, row in worst.iterrows():\n",
    "        print(\n",
    "            f\"  â€¢ Cluster {int(row['Cluster'])}: F1 = {row['F1']:.3f} (Precision = {row['Precision']:.3f}, Recall = {row['Recall']:.3f})\"\n",
    "        )\n",
    "\n",
    "    zero_f1 = df_perf[df_perf[\"F1\"] == 0]\n",
    "    if not zero_f1.empty:\n",
    "        print(f\"\\nğŸ’€ Clusters with F1 = 0:\")\n",
    "        for _, row in zero_f1.iterrows():\n",
    "            print(\n",
    "                f\"  â€¢ Cluster {int(row['Cluster'])}: (Precision = {row['Precision']:.3f}, Recall = {row['Recall']:.3f})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1501bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Final Norms: 1.0\n",
      "âœ… Cluster prediction accuracy: 0.9670\n",
      "ğŸ” Confusion matrix:\n",
      "[[403   7   0   1   0   0   2   0]\n",
      " [  6 589   0   2   2   0   3   0]\n",
      " [  0   0  59   1   0   0   0   0]\n",
      " [  2   6   0 191   0   0   1   1]\n",
      " [  0   2   0   0 152   3   0   0]\n",
      " [  0   0   0   0   4 149   1   0]\n",
      " [  2  12   0   0   0   1 241   0]\n",
      " [  1   1   0   1   0   0   0  30]]\n"
     ]
    }
   ],
   "source": [
    "# âš ï¸ Adjust for alignment: cluster_labels[window_size:] corresponds to y\n",
    "true_clusters = cluster_labels[window_size:]  # Shifted to match y[i] = E[t+window_size]\n",
    "\n",
    "# ğŸ”® Predict next embedding for all X\n",
    "predicted_embeddings = model.predict(X)  # shape: (num_samples, embedding_dim)\n",
    "print(f'Final Norms: {np.round(np.linalg.norm(predicted_embeddings, axis=1).mean(), 4)}')\n",
    "\n",
    "# ğŸ§  Predict clusters\n",
    "predicted_clusters = knn.predict(predicted_embeddings)\n",
    "\n",
    "# ğŸ§ª Accuracy\n",
    "acc = accuracy_score(true_clusters, predicted_clusters)\n",
    "print(f\"âœ… Cluster prediction accuracy: {acc:.4f}\")\n",
    "\n",
    "# ğŸ§® Confusion matrix\n",
    "cm = confusion_matrix(true_clusters, predicted_clusters)\n",
    "print(\"ğŸ” Confusion matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3db31b",
   "metadata": {},
   "source": [
    "### ğŸ”¥ Visualize the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5460252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Cluster\")\n",
    "plt.ylabel(\"True Cluster\")\n",
    "plt.title(\"Confusion Matrix â€” Predicted vs True Cluster_ID[t+1]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa9da4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_per_cluster_accuracy(cm):\n",
    "    \"\"\"\n",
    "    Print per-cluster accuracy from a given confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        cm (np.ndarray): Square confusion matrix where rows = true labels, cols = predicted labels.\n",
    "    \"\"\"\n",
    "    per_cluster_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\nğŸ¯ Per-Cluster Accuracy:\")\n",
    "    for i, acc in enumerate(per_cluster_acc):\n",
    "        print(f\"  â€¢ Cluster {i}: Accuracy = {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "118ea506",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    true_clusters, predicted_clusters, average=None, zero_division=0\n",
    ")\n",
    "df_perf = pd.DataFrame(\n",
    "    {\n",
    "        \"Cluster\": list(range(len(precision))),\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "017bb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cluster_distribution(cluster_labels, window_size, val_pct=0.2):\n",
    "    # Align cluster labels to match y[t] (since y starts after window_size)\n",
    "    cluster_labels = cluster_labels[window_size:]\n",
    "    total_len = len(cluster_labels)\n",
    "    split_idx = int((1 - val_pct) * total_len)\n",
    "\n",
    "    train_clusters = cluster_labels[:split_idx]\n",
    "    val_clusters = cluster_labels[split_idx:]\n",
    "\n",
    "    # Count unique clusters\n",
    "    train_counts = Counter(train_clusters)\n",
    "    val_counts = Counter(val_clusters)\n",
    "    all_clusters = sorted(set(cluster_labels[~np.isnan(cluster_labels)]))\n",
    "\n",
    "    print(\"ğŸ“Š Per-Cluster Sample Counts:\")\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Cluster\": all_clusters,\n",
    "            \"Train Count\": [train_counts.get(c, 0) for c in all_clusters],\n",
    "            \"Val Count\": [val_counts.get(c, 0) for c in all_clusters],\n",
    "            \"Total Count\": [\n",
    "                train_counts.get(c, 0) + val_counts.get(c, 0) for c in all_clusters\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # ğŸ”” Warn about missing clusters in training\n",
    "    missing = [c for c in all_clusters if train_counts.get(c, 0) == 0]\n",
    "    if missing:\n",
    "        print(f\"\\nğŸš¨ Warning: Clusters missing in training set: {missing}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… All clusters present in training set.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_training_run(config, cm, df_perf):\n",
    "    \"\"\"\n",
    "    Print a full summary including config, per-cluster accuracy, and F1 breakdown.\n",
    "\n",
    "    Args:\n",
    "        config (dict): {\n",
    "            \"Model Type\": ..., \"Loss\": ..., \"Epochs\": ..., etc.\n",
    "        }\n",
    "        cm (np.ndarray): Confusion matrix.\n",
    "        df_perf (pd.DataFrame): Precision, Recall, F1 per cluster.\n",
    "    \"\"\"\n",
    "    # Header\n",
    "    print(\"âš™ï¸ Training Summary:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\nğŸ” Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    print_per_cluster_accuracy(cm)\n",
    "\n",
    "    # F1 summary\n",
    "    print_summary_metrics(df_perf)\n",
    "\n",
    "    check_cluster_distribution(cluster_labels, window_size=window_size, val_pct=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc6b6a",
   "metadata": {},
   "source": [
    "### âš™ï¸ Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Summary:\n",
      "  Model Type: stratum_hydra_forecaster\n",
      "  Output Normalization: Enabled\n",
      "  Loss Function: cosine_loss\n",
      "  Validation Split: None\n",
      "  Callbacks Used: ReduceLROnPlateau\n",
      "  Epochs Run: 300\n",
      "  Final Train Loss: 0.019791\n",
      "  Embedding MSE: 0.000618\n",
      "  Predicted Probabilities: [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8]\n",
      "  Predicted Cluster: 7.0 / 0.8000\n",
      "\n",
      "ğŸ” Confusion matrix:\n",
      "[[403   7   0   1   0   0   2   0]\n",
      " [  6 589   0   2   2   0   3   0]\n",
      " [  0   0  59   1   0   0   0   0]\n",
      " [  2   6   0 191   0   0   1   1]\n",
      " [  0   2   0   0 152   3   0   0]\n",
      " [  0   0   0   0   4 149   1   0]\n",
      " [  2  12   0   0   0   1 241   0]\n",
      " [  1   1   0   1   0   0   0  30]]\n",
      "\n",
      "ğŸ¯ Per-Cluster Accuracy:\n",
      "  â€¢ Cluster 0: Accuracy = 97.58%\n",
      "  â€¢ Cluster 1: Accuracy = 97.84%\n",
      "  â€¢ Cluster 2: Accuracy = 98.33%\n",
      "  â€¢ Cluster 3: Accuracy = 95.02%\n",
      "  â€¢ Cluster 4: Accuracy = 96.82%\n",
      "  â€¢ Cluster 5: Accuracy = 96.75%\n",
      "  â€¢ Cluster 6: Accuracy = 94.14%\n",
      "  â€¢ Cluster 7: Accuracy = 90.91%\n",
      "\n",
      "âœ… Top 3 Clusters by F1 Score:\n",
      "  â€¢ Cluster 2: F1 = 0.992 (Precision = 1.000, Recall = 0.983)\n",
      "  â€¢ Cluster 0: F1 = 0.975 (Precision = 0.973, Recall = 0.976)\n",
      "  â€¢ Cluster 5: F1 = 0.971 (Precision = 0.974, Recall = 0.968)\n",
      "\n",
      "âŒ Bottom 3 Clusters by F1 Score (non-zero only):\n",
      "  â€¢ Cluster 3: F1 = 0.962 (Precision = 0.974, Recall = 0.950)\n",
      "  â€¢ Cluster 6: F1 = 0.956 (Precision = 0.972, Recall = 0.941)\n",
      "  â€¢ Cluster 7: F1 = 0.938 (Precision = 0.968, Recall = 0.909)\n",
      "ğŸ§ª Embedding MSE: 0.000618\n",
      "ğŸ“Š Per-Cluster Sample Counts:\n",
      " Cluster  Train Count  Val Count  Total Count\n",
      "     0.0          413          0          413\n",
      "     1.0          602          0          602\n",
      "     2.0           60          0           60\n",
      "     3.0          201          0          201\n",
      "     4.0          157          0          157\n",
      "     5.0          154          0          154\n",
      "     6.0          256          0          256\n",
      "     7.0           33          0           33\n",
      "\n",
      "âœ… All clusters present in training set.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§® Extract metrics from history\n",
    "epochs_run = len(history.history[\"loss\"])\n",
    "final_train_loss = history.history[\"loss\"][-1]\n",
    "final_val_loss = history.history[\"val_loss\"][-1] if USE_VALIDATION else None\n",
    "\n",
    "# ğŸ§  Format labels\n",
    "loss_name = loss_fn.__name__ if callable(loss_fn) else str(loss_fn)\n",
    "callback_names = (\n",
    "    [cb.__class__.__name__ for cb in callbacks] if \"callbacks\" in locals() else []\n",
    ")\n",
    "model_type = model.name if hasattr(model, \"name\") else \"Unknown\"\n",
    "val_pct = f\"{int(validation_split * 100)}%\" if validation_split > 0 else \"None\"\n",
    "predicted_pct = (\n",
    "    f\"{predicted_prob[0].max() * 100:.2f}%\"\n",
    "    if predicted_prob.size > 0\n",
    "    else \"N/A\"\n",
    ")\n",
    "# Embedding Prediction Error\n",
    "embedding_mse = np.mean(np.square(y - predicted_embeddings))\n",
    "\n",
    "# ğŸ“¦ Build config dict\n",
    "config = {\n",
    "    \"Model Type\": model_type,\n",
    "    \"Output Normalization\": \"Enabled\" if USE_OUTPUT_NORMALIZATION else \"Disabled\",\n",
    "    \"Loss Function\": loss_name,\n",
    "    \"Validation Split\": val_pct,\n",
    "    \"Callbacks Used\": \", \".join(callback_names) if callback_names else \"None\",\n",
    "    \"Epochs Run\": epochs_run,\n",
    "    \"Final Train Loss\": f\"{final_train_loss:.6f}\",\n",
    "    \"Embedding MSE\": f\"{embedding_mse:.6f}\",\n",
    "    \"Predicted Probabilities\": predicted_prob[0].tolist(),\n",
    "    \"Predicted Cluster\": f\"{predicted_cluster} / {predicted_pct}\",\n",
    "}\n",
    "if final_val_loss is not None:\n",
    "    config[\"Final Val Loss\"] = f\"{final_val_loss:.6f}\"\n",
    "\n",
    "# ğŸ“Š Final report\n",
    "summarize_training_run(config, cm, df_perf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
