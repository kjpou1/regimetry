{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb500211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd900e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_df = pd.read_csv(\n",
    "    \"../../artifacts/reports/GBP_USD/ws5/learnable64/default/nc8/regime_assignments_dbg.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdda31be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Unique raw Cluster_IDs: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)}\n"
     ]
    }
   ],
   "source": [
    "raw_cluster_series = regime_df[\"Cluster_ID\"].dropna().astype(int)\n",
    "raw_cluster_ids = set(raw_cluster_series.unique())\n",
    "regime_counts = raw_cluster_series.value_counts().sort_index()\n",
    "print(\"🔎 Unique raw Cluster_IDs:\", raw_cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8844e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir = Path(\"../../artifacts/baseline_metadata/GBP_USD/ws5/learnable64/default/nc8\")\n",
    "\n",
    "baseline_path = metadata_dir / \"regime_assignments_baseline.csv\"\n",
    "mapping_path = metadata_dir / \"regime_label_mapping.json\"\n",
    "\n",
    "baseline_series = pd.read_csv(baseline_path)[\"Cluster_ID\"].dropna().astype(int)\n",
    "\n",
    "\n",
    "# Load baseline assignments\n",
    "baseline_counts = baseline_series.value_counts().sort_index()\n",
    "\n",
    "\n",
    "with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    existing_mapping = json.load(f)\n",
    "    existing_mapping = {int(k): int(v) for k, v in existing_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d770880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Unique regime Cluster_IDs: Cluster_ID\n",
      "0    600\n",
      "1    162\n",
      "2    413\n",
      "3    157\n",
      "4    202\n",
      "5     60\n",
      "6     33\n",
      "7    254\n",
      "Name: count, dtype: int64\n",
      "🔎 Unique baseline Cluster_IDs: Cluster_ID\n",
      "0    604\n",
      "1    253\n",
      "2    413\n",
      "3    157\n",
      "4     60\n",
      "5     30\n",
      "6    160\n",
      "7    202\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Unique regime Cluster_IDs:\", regime_counts)\n",
    "print(\"🔎 Unique baseline Cluster_IDs:\", baseline_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f89be944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Existing mapping: {1: 0, 3: 1, 2: 2, 4: 4, 5: 5, 0: 6, 6: 7}\n",
      "🔎 Already mapped from: {0, 1, 2, 3, 4, 5, 6}\n",
      "🔎 Already mapped to: {0, 1, 2, 4, 5, 6, 7}\n",
      "🔎 Known IDs: {0, 1, 2, 3, 4, 5, 6, 7}\n",
      "🔎 Raw: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)}\n",
      "🆕 Unmapped cluster IDs: set()\n"
     ]
    }
   ],
   "source": [
    "already_mapped_from = set(existing_mapping.keys())\n",
    "already_mapped_to = set(existing_mapping.values())\n",
    "known_ids = already_mapped_from.union(already_mapped_to)\n",
    "print(\"🔎 Existing mapping:\", existing_mapping)\n",
    "print(\"🔎 Already mapped from:\", already_mapped_from)\n",
    "print(\"🔎 Already mapped to:\", already_mapped_to)\n",
    "print(\"🔎 Known IDs:\", known_ids)\n",
    "print(\"🔎 Raw:\", raw_cluster_ids)\n",
    "\n",
    "unmapped_ids = raw_cluster_ids - known_ids\n",
    "\n",
    "print(\n",
    "    \"🆕 Unmapped cluster IDs:\", unmapped_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c1a3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Masked baseline (first 10): [1 1 1 1 1 1 1 1 1 1]\n",
      "✂️ Masked current  (first 10): [7 7 7 7 7 7 7 7 7 7]\n",
      "✅ unique current: [7]\n",
      "✅ unique baseline: [0 1]\n",
      "📊 baseline_masked full unique counts:\n",
      " 1    252\n",
      "0      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(baseline_series), len(raw_cluster_series))\n",
    "baseline_trimmed = baseline_series.iloc[:min_len].to_numpy()\n",
    "current_trimmed = raw_cluster_series.iloc[:min_len].to_numpy()\n",
    "\n",
    "mask = np.isin(current_trimmed, list(unmapped_ids))\n",
    "baseline_masked = baseline_trimmed[mask]\n",
    "current_masked = current_trimmed[mask]\n",
    "\n",
    "print(\"✂️ Masked baseline (first 10):\", baseline_masked[:10])\n",
    "print(\"✂️ Masked current  (first 10):\", current_masked[:10])\n",
    "print(\"✅ unique current:\", np.unique(current_masked))\n",
    "print(\"✅ unique baseline:\", np.unique(baseline_masked))\n",
    "print(\n",
    "    \"📊 baseline_masked full unique counts:\\n\",\n",
    "    pd.Series(baseline_masked).value_counts(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25dfa496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping Hungarian — no valid overlap for unmapped IDs.\n"
     ]
    }
   ],
   "source": [
    "if len(current_masked) == 0 or not any(\n",
    "    uid in np.unique(baseline_masked) for uid in unmapped_ids\n",
    "):\n",
    "    print(\"⚠️ Skipping Hungarian — no valid overlap for unmapped IDs.\")\n",
    "else:\n",
    "    C = confusion_matrix(baseline_masked, current_masked, labels=sorted(unmapped_ids))\n",
    "    row_ind, col_ind = linear_sum_assignment(-C)\n",
    "    new_mapping = {int(col): int(row) for row, col in zip(row_ind, col_ind)}\n",
    "    print(\"📌 New Mapping:\", new_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1d29f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 All Cluster IDs: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n",
      "🔗 Confusion Matrix:\n",
      " [[599   2   0   0   0   0   1   2]\n",
      " [  0   0   0   0   1   0   0 252]\n",
      " [  0   0 413   0   0   0   0   0]\n",
      " [  0   0   0 157   0   0   0   0]\n",
      " [  0   0   0   0   0  60   0   0]\n",
      " [  0   0   0   0   0   0  30   0]\n",
      " [  0 160   0   0   0   0   0   0]\n",
      " [  1   0   0   0 201   0   0   0]]\n",
      "✅ Hungarian Mapping:\n",
      "    0 → 0\n",
      "    7 → 1\n",
      "    2 → 2\n",
      "    3 → 3\n",
      "    5 → 4\n",
      "    6 → 5\n",
      "    1 → 6\n",
      "    4 → 7\n"
     ]
    }
   ],
   "source": [
    "# Drop NaNs and convert to int\n",
    "current_series = regime_df[\"Cluster_ID\"].dropna().astype(int)\n",
    "\n",
    "# Trim to same length\n",
    "min_len = min(len(current_series), len(baseline_series))\n",
    "current_trimmed = current_series.iloc[:min_len].to_numpy()\n",
    "baseline_trimmed = baseline_series.iloc[:min_len].to_numpy()\n",
    "\n",
    "# Compute unified label space\n",
    "all_ids = sorted(set(current_trimmed) | set(baseline_trimmed))\n",
    "\n",
    "# Confusion matrix\n",
    "C = confusion_matrix(baseline_trimmed, current_trimmed, labels=all_ids)\n",
    "\n",
    "# Run Hungarian algorithm\n",
    "row_ind, col_ind = linear_sum_assignment(-C)\n",
    "mapping = {all_ids[col]: all_ids[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "# Print diagnostics\n",
    "print(\"🔢 All Cluster IDs:\", all_ids)\n",
    "print(\"🔗 Confusion Matrix:\\n\", C)\n",
    "print(\"✅ Hungarian Mapping:\")\n",
    "for k, v in mapping.items():\n",
    "    print(f\"    {k} → {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114beb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Save original raw cluster labels ---\n",
    "raw_cluster_labels = regime_df[\"Cluster_ID\"].copy()\n",
    "raw_cluster_ids = set(raw_cluster_labels.dropna().astype(int).unique())\n",
    "\n",
    "# --- Step 2: Load existing mapping ---\n",
    "# mapping_path = os.path.join(self.baseline_dir, \"regime_label_mapping.json\")\n",
    "# if os.path.exists(mapping_path):\n",
    "#     with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         existing_mapping = json.load(f)\n",
    "#         existing_mapping = {int(k): int(v) for k, v in existing_mapping.items()}\n",
    "#     logger.info(f\"🔁 Existing mapping loaded with {len(existing_mapping)} entries\")\n",
    "# else:\n",
    "#     existing_mapping = {}\n",
    "\n",
    "\n",
    "# --- Step 3: Apply existing mapping ---\n",
    "def apply_mapping(x):\n",
    "    return existing_mapping.get(int(x), x) if pd.notna(x) else x\n",
    "\n",
    "\n",
    "regime_df[\"Cluster_ID\"] = raw_cluster_labels.apply(apply_mapping)\n",
    "\n",
    "# --- Step 4: Detect unmapped cluster IDs ---\n",
    "already_mapped = set(existing_mapping.keys())\n",
    "unmapped_ids = raw_cluster_ids - already_mapped\n",
    "print(f\"🧠 Unmapped cluster IDs: {sorted(unmapped_ids)}\")\n",
    "\n",
    "# --- Step 5: Align only new IDs if baseline is present ---\n",
    "baseline_path = os.path.join(self.baseline_dir, \"regime_assignments_baseline.csv\")\n",
    "if unmapped_ids and os.path.exists(baseline_path):\n",
    "    baseline_series = pd.read_csv(baseline_path)[\"Cluster_ID\"].dropna().astype(int)\n",
    "    current_series = raw_cluster_labels.dropna().astype(int)\n",
    "\n",
    "    min_len = min(len(current_series), len(baseline_series))\n",
    "    baseline_trimmed = baseline_series.iloc[:min_len].to_numpy()\n",
    "    current_trimmed = current_series.iloc[:min_len].to_numpy()\n",
    "\n",
    "    # Restrict confusion matrix to unmapped IDs only\n",
    "    C = confusion_matrix(baseline_trimmed, current_trimmed, labels=sorted(unmapped_ids))\n",
    "\n",
    "    if C.size == 0 or C.shape[0] != C.shape[1]:\n",
    "        logger.warning(\"⚠️ Hungarian skipped: not enough overlap to align new IDs.\")\n",
    "    else:\n",
    "        row_ind, col_ind = linear_sum_assignment(-C)\n",
    "        new_mapping = {\n",
    "            int(sorted(unmapped_ids)[col]): int(sorted(unmapped_ids)[row])\n",
    "            for row, col in zip(row_ind, col_ind)\n",
    "        }\n",
    "\n",
    "        # Add new mappings to existing\n",
    "        for k, v in new_mapping.items():\n",
    "            if k not in existing_mapping:\n",
    "                existing_mapping[k] = v\n",
    "        print(f\"🔐 Updated mapping with {len(new_mapping)} new entries.\")\n",
    "\n",
    "        # Re-apply full mapping after update\n",
    "        regime_df[\"Cluster_ID\"] = raw_cluster_labels.apply(\n",
    "            lambda x: existing_mapping.get(int(x), x) if pd.notna(x) else x\n",
    "        )\n",
    "\n",
    "        # with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        #     json.dump(existing_mapping, f, indent=2)\n",
    "        # print(f\"💾 Mapping file saved: {mapping_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
